{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$$C^n(\\omega) = -(y^n \\cdot ln(\\hat y ^n) + (1-y^n)\\cdot ln(1 - \\hat y ^n))$$\n",
    "$$\\hat y = f(x) = \\frac{1}{1+exp(-z)}, \\rightarrow  z = \\omega^T\\cdot x = \\sum _i^I \\omega_i \\cdot x_i$$\n",
    "Assuming the $y^n$ vector is constant:\n",
    "$$\\frac{\\partial C^n}{\\partial \\omega _i} = -\\frac{\\partial}{\\partial \\omega _i} \\left(y^n \\cdot ln(f(x^n))\\right) - \\frac{\\partial}{\\partial \\omega _i} \\left((1-y^n)\\cdot ln(1 - f(x^n))\\right)$$\n",
    "\n",
    "Performing the partial derivation on the two seperate terms in the previous equation we get:\n",
    "\n",
    "Starting with the first term: \n",
    "$$First = -y^n \\cdot \\frac{\\partial}{\\partial \\omega _i} (ln(f(x^n))) = -y^n \\cdot \\frac{1}{f(x^n)}\\cdot \\frac{\\partial f(x^n)}{\\partial \\omega _i} = -\\frac{y^n}{f(x^n)} \\cdot \\frac{x_i^n \\cdot exp(-z)}{(1+exp(-z))^2} = -y^n \\hat y^n x_i^n \\cdot exp(-z)$$\n",
    "\n",
    "Continuing with the second term:\n",
    "$$Second = -(1-y^n)\\frac{1}{1-f(x^n)} \\cdot \\left(-\\frac{x_i^n \\cdot exp(-z)}{(1+exp(-z))^2} \\right)$$\n",
    "\n",
    "Expanding on what $1-f(x^n)$ is:\n",
    "$$ 1-f(x^n) = \\frac{1+exp(-z)}{1+exp(-z)} - \\frac{1}{1+exp(-z)} = \\frac{exp(-z)}{1+exp(-z)}$$\n",
    "Hence;\n",
    "$$\\frac{1}{1-f(x^n)} = \\frac{1+exp(-z)}{exp(-z)} = \\frac{1}{exp(-z)} \\cdot \\frac{1}{f(x^n)}$$\n",
    "\n",
    "Going back to the $second$ term:\n",
    "$$ -(1-y^n) \\frac{1}{exp(-z)} \\cdot \\frac{1}{f(x^n)} \\cdot \\left(-\\frac{x_i^n \\cdot exp(-z)}{(1+exp(-z))^2} \\right) = -(1-y^n) \\frac{1}{exp(-z)} \\cdot \\frac{1}{f(x^n)} \\cdot (-x_i^n \\cdot exp(-z) \\cdot f(x^n)^2)$$\n",
    "Hence;\n",
    "$$Second = (1-y^n)\\cdot x_i^n \\hat y^n$$\n",
    "\n",
    "This gives us:\n",
    "$$\\frac{\\partial C^n}{\\partial \\omega _i} = x_i^n \\left( -y^n \\hat y^n \\cdot exp(-z) + (1-y^n)\\cdot \\hat y^n\\right) = x_i^n \\left( -y^n \\hat y^n \\cdot exp(-z) + \\hat y^n -y^n \\hat y^n \\right) = x_i^n ( \\hat y^n - y^n \\hat y^n(1-exp(-z)))$$\n",
    "\n",
    "Seeing that $(1-exp(-z))$ = \\frac{1}{f(x^n)}, we finaly get:\n",
    "\n",
    "$$\\frac{\\partial C^n}{\\partial \\omega _i} = x_i^n(\\hat y^n - y^n) = -( y^n - \\hat y^n)x_i^n \\rightarrow QED$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "To find the partial derivative $\\frac{\\partial C^n}{\\partial \\omega _{k,j}}$ using the chain rule.\n",
    "\n",
    "$$\\frac{\\partial C^n}{\\partial \\omega _{k,j}} = \\frac{\\partial C^n}{\\partial \\hat{y}^k_n} \\frac{\\partial \\hat{y}^k_n}{\\partial z_k} \\frac{\\partial z_k}{\\partial \\omega _{k,j}}$$\n",
    "\n",
    "The first partial derivative is straightforward:\n",
    "\n",
    "$$\\frac{\\partial C^n}{\\partial \\hat{y}^k_n} = -\\sum_{k=1}^{K} y^n_k \\frac{1}{\\hat{y}^k_n} $$\n",
    "\n",
    "For the last partial derivative, we note that for all $\\omega_{k,i}$ except $\\omega _{k,j}$, the partial derivative becomes zero. Thus, we get:\n",
    "$$\\frac{\\partial z_k}{\\partial \\omega _{k,j}} = x^n_j $$\n",
    "\n",
    "For the second partial derivative, we split the derivation into two parts. In the first case, we assume $k = k'$:\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}^k_n}{\\partial z_k} = \\frac{\\exp(z_k)\\sum_{k'} \\exp(z_{k'})- \\exp(z_k)\\exp(z_k)}{(\\sum_{k'} exp(z_{k'}))^2} = \\frac{exp(z_k)}{\\sum_{k'} \\exp(z_{k'})} (\\frac{\\sum_{k'} exp(z_{k'})}{\\sum_{k'} exp(z_{k'})} - \\frac{exp(z_k)}{\\sum_{k'} exp(z_{k'})}) = \\hat y_k^n (1 - \\hat y_k^n)$$\n",
    "\n",
    "In the second case, $k \\neq k'$:\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}^n_k}{\\partial z_k} = \\frac{0 - \\exp(z_k)\\exp(z_{k'})}{(\\sum_{k'} exp(z_{k'}))^2} = -\\hat y_k^n \\hat y_{k'}^n $$\n",
    "\n",
    "Combining this, we get\n",
    "\n",
    "$$ \\frac{\\partial C^n}{\\partial \\omega _{k,j}} = \\frac{\\partial z_k}{\\partial \\omega _{k,j}}(-\\sum_{k \\neq k'} y^n_k \\frac{1}{\\hat{y}^n_k} \\frac{\\partial \\hat{y}^n_k}{\\partial z_k} - y_{k'} \\frac{1}{ \\hat y_{k'}} \\frac{\\partial \\hat{y}^n_{k'}}{\\partial z_{k'}}) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n}{\\partial \\omega _{k,j}} = x^n_j(-\\sum_{k \\neq k'} y^n_k \\frac{1}{\\hat{y}^n_k} (-\\hat y_k^n \\hat y_{k'}^n) - y_{k'}\\frac{1}{\\hat{y}^n_{k'}} \\hat y_k^n (1 - \\hat y_k^n)) $$\n",
    "\n",
    "$$ \\frac{\\partial C^n}{\\partial \\omega _{k,j}} =  x^n_j(\\sum_{k \\neq k'} y^n_k \\hat{y}^n_{k'} + y^n_k \\hat y_{k'}^n - y_{k'} = x^n_j(\\sum_{k} y^n_k \\hat{y}^n_{k'} - y_{k'}) = x^n_j(\\hat{y}^n_{k'}\\sum_{k} y^n_k - y_{k'}) = -x^n_j(y_{k'} - \\hat{y}^n_{k'})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping generelly kicks in after 30 epochs. With shuffling this number varies between 15 and 35, but is usually approximately 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "When using logistic regression with gradient descent, there is a risk of being stuck in local minima where the gradient is near zero. Without shuffling, the dataset is static and these minima will not be properly avoided. Using shuffling however, provides a way to rearrange the dataset between epochs, so a local minima will no longer be in the same place as before.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "In terms of train loss and validation loss, we observe that near the end, the train loss seems to have lower mean than the validation loss. This might be an indicator of overfitting, as the model then better describes the training data than the validation data. This means the model is not general enough, and therefore cannot account for biases in the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\omega} = \\frac{\\partial C}{\\partial \\omega} + \\frac{\\partial R}{\\partial \\omega} = \\frac{-x(y - \\hat y)}{N} + \\frac{\\lambda}{2} \\frac{\\partial}{\\partial \\omega} \\omega (\\omega)^T = \\frac{-x(y - \\hat y)}{N} + \\lambda \\omega$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
